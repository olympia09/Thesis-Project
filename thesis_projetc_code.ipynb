{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLUSTERING NON SEPARABLE DATA WITH GIBBS SAMPLING FOR GAUSSIAN MIXTURE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import dirichlet, multivariate_normal, invwishart\n",
    "from matplotlib.patches import Ellipse\n",
    "from itertools import product\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import matplotlib.colors as mcolors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for normalizing data\n",
    "\n",
    "def normalize_data(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    std = np.std(data, axis=0)\n",
    "    normalized_data = (data - mean) / std\n",
    "    return normalized_data, mean, std\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NHANES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('NHANES_adults_data_preproc.csv')\n",
    "\n",
    "# Select the columns for BMI and SBP\n",
    "df_bmi_sbp = df[['bmi', 'sbp']]\n",
    "\n",
    "# Check for missing values in the selected columns\n",
    "missing_values = df_bmi_sbp.isnull().sum()\n",
    "print('-' * 30)\n",
    "print(\"Missing values before removal:\")\n",
    "print(missing_values)\n",
    "print('-' * 30)\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_bmi_sbp_clean = df_bmi_sbp.dropna()\n",
    "\n",
    "# Normalize the clean data\n",
    "data_US = df_bmi_sbp_clean.values\n",
    "data_norm_US_full, mean_US, std_US = normalize_data(data_US)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "df_bmi_sbp_norm = pd.DataFrame(data_norm_US_full, columns=['bmi', 'sbp'])\n",
    "\n",
    "# Take a random sample of 300 from the normalized data\n",
    "data_norm_US = df_bmi_sbp_norm.sample(n=300, replace=False, random_state=5)\n",
    "\n",
    "# Plot the scatter plot of normalized BMI vs SBP\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(data_norm_US['bmi'], data_norm_US['sbp'], alpha=0.5, color='purple')\n",
    "plt.title('Scatter Plot of Normalized BMI vs SBP (NHANES Dataset Subset)')\n",
    "plt.xlabel('Normalized BMI')\n",
    "plt.ylabel('Normalized SBP (Systolic Blood Pressure)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "data_norm_US = data_norm_US.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(seed=None, means=[(-2, -2), (0, 0), (2, 2)], covariances=[np.eye(2), np.eye(2), np.eye(2)], n_samples=100):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    data = np.vstack([\n",
    "        np.random.multivariate_normal(means[0], covariances[0], n_samples),\n",
    "        np.random.multivariate_normal(means[1], covariances[1], n_samples),\n",
    "        np.random.multivariate_normal(means[2], covariances[2], n_samples)\n",
    "    ])\n",
    "    \n",
    "    true_labels = np.hstack([\n",
    "        np.zeros(n_samples),\n",
    "        np.ones(n_samples),\n",
    "        np.full(n_samples, 2)\n",
    "    ])\n",
    "    \n",
    "    return data, true_labels\n",
    "\n",
    "\n",
    "\n",
    "data, true_labels = create_data(1)\n",
    "data_norm, mean, std = normalize_data(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization based on random subset\n",
    "\n",
    "def initialize_parameters_random_subset(data, K, s=None):\n",
    "    N, D = data.shape\n",
    "    \n",
    "    if s is not None:\n",
    "        np.random.seed(s)\n",
    "    \n",
    "    # Step 1: Select K random data points as initial means\n",
    "    random_indices = np.random.choice(N, K, replace=False)\n",
    "    mu = data[random_indices]\n",
    "    \n",
    "    # Step 2: Assign initial cluster labels based on closest mean\n",
    "    z = np.zeros(N, dtype=int)\n",
    "    for i in range(N):\n",
    "        distances = np.linalg.norm(data[i] - mu, axis=1)\n",
    "        z[i] = np.argmin(distances)\n",
    "    \n",
    "    # Step 3: Initialize covariances\n",
    "    Sigma = np.zeros((K, D, D))\n",
    "    for k in range(K):\n",
    "        cluster_data = data[z == k]\n",
    "        if len(cluster_data) > 1:\n",
    "            Sigma[k] = np.cov(cluster_data, rowvar=False)\n",
    "        else:\n",
    "            Sigma[k] = np.eye(D)  # if a cluster has only one point, use identity matrix as covariance\n",
    "    \n",
    "    # Step 4: Initialize mixing coefficients\n",
    "    pi = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        pi[k] = np.sum(z == k) / N\n",
    "    \n",
    "    return z, pi, mu, Sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_z(data, pi, mu, Sigma, K):\n",
    "    N = data.shape[0]\n",
    "    z = np.zeros(N, dtype=int)\n",
    "    probabilities = np.zeros((N, K))\n",
    "    \n",
    "    for i in range(N):\n",
    "        probs = np.array([pi[k] * multivariate_normal.pdf(data[i], mean=mu[k], cov=Sigma[k]) for k in range(K)])\n",
    "        probs /= np.sum(probs)\n",
    "        z[i] = np.argmax(np.random.multinomial(1, probs))\n",
    "        probabilities[i] = probs\n",
    "\n",
    "    return z, probabilities\n",
    "\n",
    "# Computes the probability of each data point belonging to each cluster and samples a cluster assignment based on these probabilities\n",
    "\n",
    "\n",
    "\n",
    "def sample_pi(z, K, alpha):\n",
    "    counts = np.array([np.sum(z == k) for k in range(K)])\n",
    "    # counts = np.bincount(z, minlength=len(alpha))\n",
    "    pi = dirichlet.rvs(alpha + counts, size=1)\n",
    "    return pi[0]\n",
    "\n",
    "\n",
    "\n",
    "def sample_mu(data, z, K, V0, m0, Sigma):\n",
    "    N, D = data.shape\n",
    "    mu = np.zeros((K, D))\n",
    "    \n",
    "    for k in range(K):\n",
    "        Nk = np.sum(z == k)\n",
    "        if Nk > 0:\n",
    "            xk = np.mean(data[z == k], axis=0)\n",
    "            # Update V_k and m_k\n",
    "            V_k_inv = np.linalg.inv(V0) + Nk * np.linalg.inv(Sigma[k])\n",
    "            V_k = np.linalg.inv(V_k_inv)\n",
    "            m_k = V_k @ (np.linalg.inv(Sigma[k]) @ (Nk * xk) + np.linalg.inv(V0) @ m0)\n",
    "            \n",
    "            # Sample mu_k\n",
    "            mu[k] = multivariate_normal.rvs(mean=m_k, cov=V_k)\n",
    "        else:\n",
    "            # Sample from the prior distribution\n",
    "            mu[k] = multivariate_normal.rvs(mean=m0, cov=V0)\n",
    "    \n",
    "    return mu\n",
    "\n",
    "\n",
    "def sample_Sigma(data, z, K, mu, S0, nu0):\n",
    "    N, D = data.shape\n",
    "    Sigma = np.zeros((K, D, D))\n",
    "    \n",
    "    for k in range(K):\n",
    "        Nk = np.sum(z == k)\n",
    "        \n",
    "        # Update S_k and nu_k\n",
    "        if Nk > 0:\n",
    "            S_k = S0 + np.sum([np.outer(data[i] - mu[k], data[i] - mu[k]) for i in range(N) if z[i] == k], axis=0)\n",
    "        else:\n",
    "            S_k = S0  # Sample from prior if Nk == 0\n",
    "            \n",
    "        nu_k = nu0 + Nk\n",
    "        \n",
    "        # Sample Sigma_k\n",
    "        Sigma[k] = invwishart.rvs(df=nu_k, scale=S_k)\n",
    "\n",
    "    return Sigma\n",
    "\n",
    "\n",
    "# Log likelihood \n",
    "\n",
    "def compute_log_likelihood(data, pi, mu, Sigma, K):\n",
    "    N = data.shape[0]\n",
    "    log_likelihood = 0\n",
    "    for i in range(N):\n",
    "        likelihood_i = 0\n",
    "        for k in range(K):\n",
    "            likelihood_i += pi[k] * multivariate_normal.pdf(data[i], mean=mu[k], cov=Sigma[k])\n",
    "        log_likelihood += np.log(likelihood_i)\n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "\n",
    "def gibbs_sampling_gmm(data, K, alpha, m0, V0, S0, nu0, initialization_function, num_iterations, s=None):\n",
    "    N, D = data.shape\n",
    "    \n",
    "    # Initialize variables\n",
    "    z, pi, mu, Sigma = initialization_function(data, K, s)\n",
    "\n",
    "    \n",
    "    samples = {'z': [], 'probabilities': [],'pi': [],'mu': [],'Sigma': [], 'log_likelihood': []}\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Sample z\n",
    "        z, probabilities = sample_z(data, pi, mu, Sigma, K)\n",
    "        \n",
    "        # Sample pi\n",
    "        pi = sample_pi(z, K, alpha)\n",
    "        \n",
    "        # Sample mu\n",
    "        mu = sample_mu(data, z, K, V0, m0, Sigma)\n",
    "        \n",
    "        # Sample Sigma\n",
    "        Sigma = sample_Sigma(data, z, K, mu, S0, nu0)\n",
    "\n",
    "        log_likelihood = compute_log_likelihood(data, pi, mu, Sigma, K)\n",
    "        samples['log_likelihood'].append(log_likelihood)\n",
    "        # Append current samples to the list\n",
    "        samples['z'].append(z.copy())\n",
    "        samples['probabilities'].append(probabilities.copy())\n",
    "        samples['pi'].append(pi.copy())\n",
    "        samples['mu'].append(mu.copy())\n",
    "        samples['Sigma'].append(Sigma.copy())\n",
    "    \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_trace_plots(samples, num_iterations, K, S0, nu0):\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(8, 14))\n",
    "\n",
    "    # Plot trace of pi\n",
    "    for k in range(K):\n",
    "        axes[0].plot([samples['pi'][i][k] for i in range(num_iterations)], label=f'pi_{k}')\n",
    "    axes[0].set_title(f'Trace plot of pi with S0={S0}, nu0={nu0}')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot trace of mu\n",
    "    for k in range(K):\n",
    "        for d in range(samples['mu'][0].shape[1]):\n",
    "            axes[1].plot([samples['mu'][i][k, d] for i in range(num_iterations)], label=f'mu_{k}_{d}')\n",
    "    axes[1].set_title('Trace plot of mu with S0={S0}, nu0={nu0}')\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Plot trace of Sigma (diagonal elements only)\n",
    "    for k in range(K):\n",
    "        for d in range(samples['Sigma'][0].shape[1]):\n",
    "            axes[2].plot([samples['Sigma'][i][k, d, d] for i in range(num_iterations)], label=f'Sigma_{k}_{d}_{d}')\n",
    "    axes[2].set_title('Trace plot of Sigma (diagonal elements) with S0={S0}, nu0={nu0}')\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_ellipse(ax, mu, cov, color):\n",
    "    \"\"\"\n",
    "    Plot an ellipse representing the covariance matrix.\n",
    "    \"\"\"\n",
    "    # Eigenvalues and eigenvectors of the covariance matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "    \n",
    "    # Order the eigenvalues and eigenvectors by descending eigenvalues\n",
    "    order = eigenvalues.argsort()[::-1]\n",
    "    eigenvalues = eigenvalues[order]\n",
    "    eigenvectors = eigenvectors[:, order]\n",
    "    \n",
    "    # Angle of the ellipse\n",
    "    angle = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))\n",
    "    \n",
    "    # Width and height of the ellipse (2 standard deviations)\n",
    "    width, height = 2 * 2 * np.sqrt(eigenvalues)\n",
    "    \n",
    "    # Plot the ellipse\n",
    "    ellipse = Ellipse(xy=mu, width=width, height=height, angle=angle, edgecolor=color, facecolor='none')\n",
    "    ax.add_patch(ellipse)\n",
    "\n",
    "\n",
    "\n",
    "def plot_log_likelihood(samples, title):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.plot(samples['log_likelihood'])\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Iteration', fontsize=13)\n",
    "    plt.ylabel('Log-Likelihood', fontsize=13)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fixed_palette(selected_indices, table='tab10'):\n",
    "    colormap = plt.get_cmap(table)\n",
    "    fixed_palette = np.array([colormap(i)[:3] for i in selected_indices])\n",
    "    return fixed_palette\n",
    "\n",
    "def probabilities_to_rgb(probabilities, fixed_palette):\n",
    "    # probabilities: Array of shape (N, K) representing probabilities for N data points and K clusters\n",
    "    # fixed_palette: Array of shape (K, 3) representing RGB colors for K clusters\n",
    "    rgb_colors = probabilities @ fixed_palette\n",
    "    return rgb_colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iterations_rgb(samples, iterations, data, K, fixed_palette, title, x_axis, y_axis):\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    fig.suptitle(title, fontsize=17)\n",
    "    \n",
    "    for i, iteration in enumerate(iterations):\n",
    "        z = samples['z'][iteration]\n",
    "        probabilities = samples['probabilities'][iteration]\n",
    "        mu = samples['mu'][iteration]\n",
    "        Sigma = samples['Sigma'][iteration]\n",
    "        rgb_colors = probabilities_to_rgb(probabilities, fixed_palette)\n",
    "        \n",
    "        row, col = divmod(i, 2)\n",
    "        scatter = ax[row, col].scatter(data[:, 0], data[:, 1], c=rgb_colors, marker='o')\n",
    "        ax[row, col].set_title(f'Clustered Data at Iteration {iteration}', fontsize=15)\n",
    "        ax[row, col].set_xlabel(x_axis, fontsize=12)\n",
    "        ax[row, col].set_ylabel(y_axis, fontsize=12)\n",
    "        \n",
    "        for k in range(K):\n",
    "            ax[row, col].scatter(mu[k, 0], mu[k, 1], c='black', s=200, marker='x', linewidths=4.5)\n",
    "            ax[row, col].scatter(mu[k, 0], mu[k, 1], c=[fixed_palette[k]], s=120, marker='x', linewidths=2.0)\n",
    "            plot_ellipse(ax[row, col], mu[k], Sigma[k], color=fixed_palette[k])\n",
    "    \n",
    "    # Add continuous color bar \n",
    "    norm = Normalize(vmin=0, vmax=1)\n",
    "    cbar = plt.colorbar(ScalarMappable(norm=norm, cmap=LinearSegmentedColormap.from_list(\"custom_cmap\", fixed_palette)), ax=ax[0, 1], orientation='vertical')\n",
    "    cbar.set_ticks([])  \n",
    "    cbar.set_label('Probability of Cluster Membership', fontsize=14)\n",
    "  \n",
    "    legend_handles = []\n",
    "    for k in range(K):\n",
    "        handle = plt.Line2D([0], [0], marker='x', color=fixed_palette[k], markersize=10, linestyle='None', label=f'Cluster {k + 1} Mean')\n",
    "        legend_handles.append(handle)\n",
    "    \n",
    "    ax[1, 1].legend(handles=legend_handles, loc='upper right', prop={'size': 10})\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPLEMENTATION OF GIBBS SAMPLER FOR GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the Gibbs sampling\n",
    "K = 3\n",
    "alpha = np.ones(K)\n",
    "m0 = np.zeros(2)\n",
    "V0 = np.eye(2)\n",
    "nu0 = 4\n",
    "num_iterations = 3000\n",
    "\n",
    "selected_indices = [0, 2, 1] \n",
    "fixed_palette = create_fixed_palette(selected_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Simulated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulated data with true labels\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "corrected_palette = fixed_palette.copy()\n",
    "corrected_palette[1], corrected_palette[2] = fixed_palette[2], fixed_palette[1]\n",
    "cmap = mcolors.ListedColormap(corrected_palette)\n",
    "norm = mcolors.BoundaryNorm(boundaries=np.arange(len(corrected_palette) + 1) - 0.5, ncolors=len(corrected_palette))\n",
    "scatter = plt.scatter(data_norm[:, 0], data_norm[:, 1], c=true_labels, cmap=cmap, norm=norm, marker='o')\n",
    "plt.title('Scatter Plot of Simulated Data with True Labels', fontsize=15)\n",
    "plt.xlabel('Feature 1', fontsize=13)\n",
    "plt.ylabel('Feature 2', fontsize=13)\n",
    "\n",
    "cbar_palette = [corrected_palette[0], corrected_palette[2], corrected_palette[1]]\n",
    "cbar_cmap = mcolors.ListedColormap(cbar_palette)\n",
    "cbar_norm = mcolors.BoundaryNorm(boundaries=np.arange(len(cbar_palette) + 1) - 0.5, ncolors=len(cbar_palette))\n",
    "\n",
    "cbar = plt.colorbar(ScalarMappable(cmap=cbar_cmap, norm=cbar_norm), ax=scatter.axes, ticks=[0, 1, 2])\n",
    "cbar.ax.set_yticklabels(['Cluster 1', 'Cluster 2', 'Cluster 3'], fontsize=11)  \n",
    "cbar.set_label('True Cluster Labels', fontsize=13)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 =  np.cov(data_norm, rowvar=False)\n",
    "# Run Gibbs sampling\n",
    "samples_gibbs = gibbs_sampling_gmm(data_norm, K, alpha, m0, V0, S0, nu0, initialize_parameters_random_subset, num_iterations=num_iterations, s=1)\n",
    "\n",
    "# Get the final clustering result\n",
    "z_gibbs = samples_gibbs['z'][-1]\n",
    "final_mu_gibbs = samples_gibbs['mu'][-1]\n",
    "final_Sigma_gibbs = samples_gibbs['Sigma'][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_likelihood(samples_gibbs, 'Log-Likelihood vs Iterations - Simulated Data')\n",
    "plot_iterations_rgb(samples_gibbs, [450, 1000, 2400, 2999], data_norm, K, fixed_palette, 'Clustered Data Across Iterations - Simulated Data', 'Feature 1', 'Feature 2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For NHANES dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 =  np.cov(data_norm_US, rowvar=False)\n",
    "\n",
    "samples_gibbs_US = gibbs_sampling_gmm(data_norm_US, K, alpha, m0, V0, S0, nu0, initialize_parameters_random_subset, num_iterations=num_iterations, s=7)\n",
    "\n",
    "z_gibbs_US = samples_gibbs_US['z'][-1]\n",
    "final_mu_gibbs_US = samples_gibbs_US['mu'][-1]\n",
    "final_Sigma_gibbs_US = samples_gibbs_US['Sigma'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_likelihood(samples_gibbs_US, 'Log-Likelihood vs Iterations - NHANES Dataset')\n",
    "plot_iterations_rgb(samples_gibbs_US, [350, 900, 1200, 2990], data_norm_US, K, fixed_palette, 'Clustered Data Across Iterations - NHANES Dataset', 'BMI', 'SBP (Systolic Blood Pressure)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_posterior(data, samples, K, fixed_palette, S0, nu0, title, x_axis, y_axis, burn_in=100):\n",
    "    mu_samples = np.array(samples['mu'][burn_in:])\n",
    "    Sigma_samples = np.array(samples['Sigma'][burn_in:])\n",
    "    pi_samples = np.array(samples['pi'][burn_in:])\n",
    "    posterior_mu = np.mean(mu_samples, axis=0)\n",
    "    posterior_Sigma = np.mean(Sigma_samples, axis=0)\n",
    "    posterior_pi = np.mean(pi_samples, axis=0)\n",
    "    \n",
    "    # Print the posterior pi with cluster labels\n",
    "    for k in range(K):\n",
    "        print(f\"Posterior pi for cluster {k+1}: {posterior_pi[k]}\")\n",
    "\n",
    "    # Calculate and print the minimum distance between cluster means\n",
    "    min_distance = np.inf\n",
    "    for i in range(K):\n",
    "        for j in range(i+1, K):\n",
    "            distance = np.linalg.norm(posterior_mu[i] - posterior_mu[j])\n",
    "            min_distance = min(min_distance, distance)\n",
    "    print(f\"Minimum distance between cluster means: {min_distance:.3f}\")\n",
    "\n",
    "    _, probabilities_posterior = sample_z(data, posterior_pi, posterior_mu, posterior_Sigma, K)\n",
    "    rgb_colors_posterior = probabilities_to_rgb(probabilities_posterior, fixed_palette)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=rgb_colors_posterior, marker='o')\n",
    "\n",
    "    for k in range(K):\n",
    "        plt.scatter(posterior_mu[k, 0], posterior_mu[k, 1], c='black', marker='x', s=250, linewidths=4.5)\n",
    "        plt.scatter(posterior_mu[k, 0], posterior_mu[k, 1], c=[fixed_palette[k]], marker='x', s=120, linewidths=2.0)\n",
    "        plot_ellipse(plt.gca(), posterior_mu[k], posterior_Sigma[k], color=fixed_palette[k])\n",
    "\n",
    "    for k in range(K):\n",
    "        plt.scatter([], [], c=[fixed_palette[k]], marker='x', s=120, label=f'Cluster {k+1} Mean')\n",
    "\n",
    "    plt.title(\n",
    "    f'Clustered {title} Data: Posterior Mean of $\\mu$ '\n",
    "    f'($\\\\nu_0$={nu0}, $S_0$=[[{int(S0[0,0])}, {int(S0[0,1])}], '\n",
    "    f'[{int(S0[1,0])}, {int(S0[1,1])}]])', fontsize=15)\n",
    "\n",
    "    plt.xlabel(x_axis, fontsize=13)\n",
    "    plt.ylabel(y_axis, fontsize=13)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = np.eye(2)\n",
    "nu0 = 4\n",
    "num_iterations = 3000\n",
    "\n",
    "# Run the Gibbs sampling\n",
    "samples_1 = gibbs_sampling_gmm(data_norm, K, alpha, m0, V0, S0, nu0, initialize_parameters_random_subset, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_posterior(data_norm, samples_1, K, fixed_palette, S0, nu0, 'Simulated', 'Feature 1', 'Feature 2', burn_in=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = np.eye(2)\n",
    "nu0 = 50\n",
    "\n",
    "samples_2 = gibbs_sampling_gmm(data_norm, K, alpha, m0, V0, S0, nu0, initialize_parameters_random_subset, num_iterations=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_posterior(data_norm, samples_2, K, fixed_palette, S0, nu0, 'Simulated', 'Feature 1', 'Feature 2', burn_in=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = 6*np.eye(2)\n",
    "nu0 = 4\n",
    "\n",
    "# Run the Gibbs sampling \n",
    "samples_3 = gibbs_sampling_gmm(data_norm, K, alpha, m0, V0, S0, nu0, initialize_parameters_random_subset, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_posterior(data_norm, samples_3, K, fixed_palette, S0, nu0, 'Simulated', 'Feature 1', 'Feature 2', burn_in=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = 6*np.eye(2)\n",
    "nu0 = 50\n",
    "\n",
    "samples_4 = gibbs_sampling_gmm(data_norm, K, alpha, m0, V0, S0, nu0, initialize_parameters_random_subset, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_posterior(data_norm, samples_4, K, fixed_palette, S0, nu0, 'Simulated', 'Feature 1', 'Feature 2', burn_in=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NHANES Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = np.eye(2)\n",
    "nu0 = 4\n",
    "\n",
    "samples_1_US = gibbs_sampling_gmm(data_norm_US, K, alpha, m0, V0, S0, nu0, initialize_parameters_random_subset, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_posterior(data_norm_US, samples_1_US, K, fixed_palette, S0, nu0, 'NHANES', 'BMI', 'SBP', burn_in=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = 6 * np.eye(2)\n",
    "nu0 = 50\n",
    "\n",
    "# Run the Gibbs sampling and store the samples\n",
    "samples_2_US = gibbs_sampling_gmm(data_norm_US, K, alpha, m0, V0, S0, nu0, initialize_parameters_random_subset, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clusters_posterior(data_norm_US, samples_2_US, K, fixed_palette, S0, nu0, 'NHANES', 'BMI', 'SBP', burn_in=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_iterations(samples, iterations, data, K, fixed_palette, S0, nu0, title, x_axis, y_axis):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))  \n",
    "    \n",
    "    for i, iteration in enumerate(iterations):\n",
    "        z = samples['z'][iteration]\n",
    "        probabilities = samples['probabilities'][iteration]\n",
    "        mu = samples['mu'][iteration]\n",
    "        Sigma = samples['Sigma'][iteration]\n",
    "        rgb_colors = probabilities_to_rgb(probabilities, fixed_palette)\n",
    "        \n",
    "        ax[i].scatter(data[:, 0], data[:, 1], c=rgb_colors, marker='o')\n",
    "        ax[i].set_title(f'Clustered Data at Iteration {iteration}', fontsize=15)\n",
    "        ax[i].set_xlabel(x_axis, fontsize=11)\n",
    "        ax[i].set_ylabel(y_axis, fontsize=11)\n",
    "        \n",
    "        # Plot the cluster means and ellipses\n",
    "        for k in range(K):\n",
    "            ax[i].scatter(mu[k, 0], mu[k, 1], c='black', s=250, marker='x', linewidths=4.5)\n",
    "            ax[i].scatter(mu[k, 0], mu[k, 1], c=[fixed_palette[k]], s=120, marker='x', linewidths=2.0)\n",
    "            plot_ellipse(ax[i], mu[k], Sigma[k], color=fixed_palette[k])\n",
    "        \n",
    "        # Compute the minimum distance between cluster means\n",
    "        min_distance = np.inf\n",
    "        for m in range(K):\n",
    "            for n in range(m + 1, K):\n",
    "                distance = np.linalg.norm(mu[m] - mu[n])\n",
    "                min_distance = min(min_distance, distance)\n",
    "        \n",
    "        print(f'Minimum distance between cluster means at iteration {iteration}: {min_distance:.3f}')\n",
    "        \n",
    "        if i == 0:\n",
    "            legend_handles = []\n",
    "            for k in range(K):\n",
    "                handle = plt.Line2D([0], [0], marker='x', color=fixed_palette[k], markersize=10, linestyle='None', label=f'Cluster {k + 1} Mean')\n",
    "                legend_handles.append(handle)\n",
    "            ax[i].legend(handles=legend_handles, loc='upper right', prop={'size': 12})\n",
    "\n",
    "    fig.suptitle(title, fontsize=18)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1]) \n",
    "     \n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraint function h1\n",
    "def h1(mu):\n",
    "    K = len(mu)\n",
    "    min_distance = float('inf')\n",
    "    for k in range(K):\n",
    "        for k_prime in range(k + 1, K):\n",
    "            distance = np.linalg.norm(mu[k] - mu[k_prime])\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "    return min_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mu_metropolis_h1(data, z, K, V0, m0, Sigma, mu_current, sigma_proposal, h):\n",
    "    N, D = data.shape\n",
    "    mu_proposed = np.zeros((K, D))\n",
    "    \n",
    "    # Proposal distr (multivariate normal distribution)\n",
    "    for k in range(K):\n",
    "        mu_proposed[k] = multivariate_normal.rvs(mean=mu_current[k], cov=sigma_proposal * np.eye(D))\n",
    "    # Compute the log-posterior for the current and proposed means\n",
    "    log_posterior_current = 0\n",
    "    log_posterior_proposed = 0\n",
    "    for k in range(K):\n",
    "        # Log-prior\n",
    "        log_posterior_current += multivariate_normal.logpdf(mu_current[k], mean=m0, cov=V0)\n",
    "        log_posterior_proposed += multivariate_normal.logpdf(mu_proposed[k], mean=m0, cov=V0)\n",
    "        # Log-likelihood\n",
    "        for i in range(N):\n",
    "            if z[i] == k:\n",
    "                log_posterior_current += multivariate_normal.logpdf(data[i], mean=mu_current[k], cov=Sigma[k])\n",
    "                log_posterior_proposed +=  multivariate_normal.logpdf(data[i], mean=mu_proposed[k], cov=Sigma[k])\n",
    "    # Add separation constraint term\n",
    "    h_current = h(mu_current)\n",
    "    h_proposed = h(mu_proposed)\n",
    "\n",
    "    log_posterior_current += np.log(h_current) \n",
    "    log_posterior_proposed += np.log(h_proposed) \n",
    "    # Compute the acceptance ratio\n",
    "    acceptance_ratio = log_posterior_proposed - log_posterior_current\n",
    "    # Accept or reject the proposal\n",
    "    if  np.log(np.random.uniform(0, 1)) <= acceptance_ratio:\n",
    "        mu = mu_proposed\n",
    "    else:\n",
    "        mu = mu_current\n",
    "    \n",
    "    return mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampling_gmm_h1(data, K, alpha, m0, V0, S0, nu0, initialization_function, sigma_proposal, num_iterations, h, s=None):\n",
    "    N, D = data.shape\n",
    "    \n",
    "    # Initialize variables\n",
    "    z, pi, mu, Sigma = initialization_function(data, K, s)\n",
    "\n",
    "    samples = {'z': [], 'probabilities': [], 'pi': [], 'mu': [], 'Sigma': [], 'log_likelihood': []}\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Sample z\n",
    "        z, probabilities = sample_z(data, pi, mu, Sigma, K)\n",
    "        \n",
    "        # Sample pi\n",
    "        pi = sample_pi(z, K, alpha)\n",
    "        \n",
    "        # Sample mu using Metropolis within Gibbs\n",
    "        mu = sample_mu_metropolis_h1(data, z, K, V0, m0, Sigma, mu, sigma_proposal, h)\n",
    "        \n",
    "        # Sample Sigma\n",
    "        Sigma = sample_Sigma(data, z, K, mu, S0, nu0)\n",
    "\n",
    "        # Store log-likelihood\n",
    "        log_likelihood = compute_log_likelihood(data, pi, mu, Sigma, K)\n",
    "        samples['log_likelihood'].append(log_likelihood)\n",
    "\n",
    "        # Append current samples to the list\n",
    "        samples['z'].append(z.copy())\n",
    "        samples['probabilities'].append(probabilities.copy())\n",
    "        samples['pi'].append(pi.copy())\n",
    "        samples['mu'].append(mu.copy())\n",
    "        samples['Sigma'].append(Sigma.copy())\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters_posterior_h(data, samples, K, fixed_palette, S0, nu0, title, x_axis, y_axis, burn_in=100):\n",
    "    mu_samples = np.array(samples['mu'][burn_in:])\n",
    "    Sigma_samples = np.array(samples['Sigma'][burn_in:])\n",
    "    pi_samples = np.array(samples['pi'][burn_in:])\n",
    "    posterior_mu = np.mean(mu_samples, axis=0)\n",
    "    posterior_Sigma = np.mean(Sigma_samples, axis=0)\n",
    "    posterior_pi = np.mean(pi_samples, axis=0)\n",
    "    \n",
    "    # Print the posterior pi with cluster labels\n",
    "    for k in range(K):\n",
    "        print(f\"Posterior pi for cluster {k+1}: {posterior_pi[k]}\")\n",
    "\n",
    "    # Calculate and print the minimum distance between cluster means\n",
    "    min_distance = np.inf\n",
    "    for i in range(K):\n",
    "        for j in range(i+1, K):\n",
    "            distance = np.linalg.norm(posterior_mu[i] - posterior_mu[j])\n",
    "            min_distance = min(min_distance, distance)\n",
    "    print(f\"Minimum distance between cluster means: {min_distance:.3f}\")\n",
    "\n",
    "    _, probabilities_posterior = sample_z(data, posterior_pi, posterior_mu, posterior_Sigma, K)\n",
    "    rgb_colors_posterior = probabilities_to_rgb(probabilities_posterior, fixed_palette)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=rgb_colors_posterior, marker='o')\n",
    "\n",
    "    for k in range(K):\n",
    "        plt.scatter(posterior_mu[k, 0], posterior_mu[k, 1], c='black', marker='x', s=250, linewidths=4.5)\n",
    "        plt.scatter(posterior_mu[k, 0], posterior_mu[k, 1], c=[fixed_palette[k]], marker='x', s=120, linewidths=2.0)\n",
    "        plot_ellipse(plt.gca(), posterior_mu[k], posterior_Sigma[k], color=fixed_palette[k])\n",
    "\n",
    "    for k in range(K):\n",
    "        plt.scatter([], [], c=[fixed_palette[k]], marker='x', s=120, label=f'Cluster {k+1} Mean')\n",
    "\n",
    "    plt.title(title, fontsize=18)\n",
    "\n",
    "    plt.xlabel(x_axis, fontsize=13)\n",
    "    plt.ylabel(y_axis, fontsize=13)\n",
    "    plt.legend(fontsize=12)\n",
    "\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu0 = 4\n",
    "S0 = np.eye(2)\n",
    "num_iterations = 3000\n",
    "sigma_proposal = 0.001\n",
    "\n",
    "samples_h1 = gibbs_sampling_gmm_h1(data_norm_US, K, alpha, m0, V0, S0, nu0, initialize_parameters_random_subset, sigma_proposal, num_iterations=num_iterations, h=h1)\n",
    "\n",
    "# Get the final clustering result\n",
    "z = samples_h1['z'][-1]\n",
    "final_mu = samples_h1['mu'][-1]\n",
    "final_Sigma = samples_h1['Sigma'][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_iterations(samples_h1, [350, 2990], data_norm_US, K, fixed_palette, S0, nu0, '\\'Minimum Distance Maximization between Cluster Centers\\' constraint (NHANES Dataset)', 'BMI', 'SBP')\n",
    "plot_clusters_posterior_h(data_norm_US, samples_h1, K, fixed_palette, S0, nu0, \"Posterior Means of Cluster Centers under \\n\\'Minimum Distance Maximization\\' constraint (NHANES Dataset)\", 'BMI', 'SBP', burn_in=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu0 = 50\n",
    "S0 = 6*np.eye(2)\n",
    "\n",
    "samples_h1_2 = gibbs_sampling_gmm_h1(data_norm_US, K, alpha, m0, V0, S0, nu0, initialize_parameters_random_subset, sigma_proposal, num_iterations=3000, h=h1)\n",
    "\n",
    "plot_two_iterations(samples_h1_2, [15, 2800], data_norm_US, K, fixed_palette, S0, nu0, 'Incorporating Constraint h1 on NHANES dataset', 'BMI', 'SBP')\n",
    "plot_clusters_posterior(data_norm_US, samples_h1_2, K, fixed_palette, S0, nu0, 'NHANES', 'BMI', 'SBP', burn_in=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h2(mu, sbp_ranges, bmi_ranges):\n",
    "    K = len(mu)  \n",
    "    blocks_assigned = set()  \n",
    "    \n",
    "    for k in range(K):\n",
    "        bmi = mu[k, 0]  \n",
    "        sbp = mu[k, 1]  \n",
    "        \n",
    "        block_valid = False  # To check if a valid block is found\n",
    "        \n",
    "        # Check SBP validity\n",
    "        for sbp_idx, (sbp_low, sbp_high) in enumerate(sbp_ranges):\n",
    "            if sbp_low <= sbp < sbp_high:\n",
    "                # Check BMI validity\n",
    "                for bmi_idx, (bmi_low, bmi_high) in enumerate(bmi_ranges):\n",
    "                    if bmi_low <= bmi < bmi_high:\n",
    "                        block = (sbp_idx, bmi_idx)\n",
    "                        if block not in blocks_assigned:  # Ensure the block is unique\n",
    "                            blocks_assigned.add(block)\n",
    "                            block_valid = True\n",
    "                            break  # Break out of BMI range loop if valid block is found\n",
    "                if block_valid:\n",
    "                    break  # Break out of SBP range loop if valid block is found\n",
    "        \n",
    "        if not block_valid:\n",
    "            return 0  # Return 0 if no valid block is found\n",
    "    \n",
    "    return 1  # Return 1 if all cluster means are valid\n",
    "\n",
    "#example to check\n",
    "mu = np.array([\n",
    "    [-0.6, 0.4], \n",
    "    [0.3, 1.0],  \n",
    "    [0.7, 1.4]    \n",
    "])\n",
    "\n",
    "sbp_ranges = [(-np.inf, 0.5), (0.5, 1.5), (1.5, np.inf)]\n",
    "bmi_ranges = [(-np.inf, -0.5), (-0.5, 0.5), (0.5, np.inf)]\n",
    "\n",
    "\n",
    "validity = h2(mu, sbp_ranges, bmi_ranges)\n",
    "print(validity)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters with valid means\n",
    "def initialize_parameters_random_subset_blocks(data, K, sbp_ranges, bmi_ranges, s=None):\n",
    "    if s is not None:\n",
    "        np.random.seed(s)\n",
    "    N, D = data.shape\n",
    "    mu = np.zeros((K, D))\n",
    "    valid = False\n",
    "    \n",
    "    while not valid:\n",
    "        random_indices = np.random.choice(N, K, replace=False)\n",
    "        mu = data[random_indices]\n",
    "        \n",
    "        if h2(mu, sbp_ranges, bmi_ranges) == 1:\n",
    "            valid = True\n",
    "    \n",
    "    z = np.zeros(N, dtype=int)\n",
    "    for i in range(N):\n",
    "        distances = np.linalg.norm(data[i] - mu, axis=1)\n",
    "        z[i] = np.argmin(distances)\n",
    "    \n",
    "    Sigma = np.zeros((K, D, D))\n",
    "    for k in range(K):\n",
    "        cluster_data = data[z == k]\n",
    "        if len(cluster_data) > 1:\n",
    "            Sigma[k] = np.cov(cluster_data, rowvar=False)\n",
    "        else:\n",
    "            Sigma[k] = np.eye(D)\n",
    "    \n",
    "    pi = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        pi[k] = np.sum(z == k) / N\n",
    "    \n",
    "    return z, pi, mu, Sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cutoff_lines(ax, sbp_cutoffs, bmi_cutoffs):\n",
    "    # Add vertical lines for BMI cutoffs\n",
    "    for (low, high) in bmi_cutoffs:\n",
    "        if low != -np.inf:\n",
    "            ax.axvline(x=low, color='green', linestyle='--')\n",
    "        if high != np.inf:\n",
    "            ax.axvline(x=high, color='green', linestyle='--')\n",
    "    \n",
    "    # Add horizontal lines for SBP cutoffs\n",
    "    for (low, high) in sbp_cutoffs:\n",
    "        if low != -np.inf:\n",
    "            ax.axhline(y=low, color='blue', linestyle='--')\n",
    "        if high != np.inf:\n",
    "            ax.axhline(y=high, color='blue', linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "def plot_two_iterations_lines(samples, iterations, data, K, fixed_palette, S0, nu0, title, x_axis, y_axis, sbp_cutoffs, bmi_cutoffs):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))  \n",
    "    \n",
    "    for i, iteration in enumerate(iterations):\n",
    "        z = samples['z'][iteration]\n",
    "        probabilities = samples['probabilities'][iteration]\n",
    "        mu = samples['mu'][iteration]\n",
    "        Sigma = samples['Sigma'][iteration]\n",
    "        rgb_colors = probabilities_to_rgb(probabilities, fixed_palette)\n",
    "        \n",
    "        ax[i].scatter(data[:, 0], data[:, 1], c=rgb_colors, marker='o')\n",
    "        ax[i].set_title(f'Clustered Data at Iteration {iteration} \\n($\\\\nu_0$={nu0}, $S_0$=[[{int(S0[0,0])}, {int(S0[0,1])}]])', fontsize=15)\n",
    "        ax[i].set_xlabel(x_axis, fontsize=11)\n",
    "        ax[i].set_ylabel(y_axis, fontsize=11)\n",
    "        \n",
    "        # Add cutoff lines\n",
    "        add_cutoff_lines(ax[i], sbp_cutoffs, bmi_cutoffs)\n",
    "        \n",
    "        # Plot the cluster means and ellipses\n",
    "        for k in range(K):\n",
    "            ax[i].scatter(mu[k, 0], mu[k, 1], c='black', s=250, marker='x', linewidths=4.5)\n",
    "            ax[i].scatter(mu[k, 0], mu[k, 1], c=[fixed_palette[k]], s=120, marker='x', linewidths=2.0)\n",
    "            plot_ellipse(ax[i], mu[k], Sigma[k], color=fixed_palette[k])\n",
    "        \n",
    "        # Compute the minimum distance between cluster means\n",
    "        min_distance = np.inf\n",
    "        for m in range(K):\n",
    "            for n in range(m + 1, K):\n",
    "                distance = np.linalg.norm(mu[m] - mu[n])\n",
    "                min_distance = min(min_distance, distance)\n",
    "        \n",
    "        print(f'Minimum distance between cluster means at iteration {iteration}: {min_distance:.3f}')\n",
    "        \n",
    "        if i == 0:\n",
    "            legend_handles = []\n",
    "            for k in range(K):\n",
    "                handle = plt.Line2D([0], [0], marker='x', color=fixed_palette[k], markersize=10, linestyle='None', label=f'Cluster {k} Mean')\n",
    "                legend_handles.append(handle)\n",
    "            ax[i].legend(handles=legend_handles, loc='upper right', prop={'size': 12})\n",
    "\n",
    "\n",
    "    fig.suptitle(title, fontsize=18)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  \n",
    "    plt.show()\n",
    "\n",
    "def plot_clusters_posterior_h_lines(data, samples, K, fixed_palette, S0, nu0, title, x_axis, y_axis, sbp_cutoffs, bmi_cutoffs, burn_in=100):\n",
    "    mu_samples = np.array(samples['mu'][burn_in:])\n",
    "    Sigma_samples = np.array(samples['Sigma'][burn_in:])\n",
    "    pi_samples = np.array(samples['pi'][burn_in:])\n",
    "    posterior_mu = np.mean(mu_samples, axis=0)\n",
    "    posterior_Sigma = np.mean(Sigma_samples, axis=0)\n",
    "    posterior_pi = np.mean(pi_samples, axis=0)\n",
    "    \n",
    "    # Print the posterior pi with cluster labels\n",
    "    for k in range(K):\n",
    "        print(f\"Posterior pi for cluster {k+1}: {posterior_pi[k]}\")\n",
    "\n",
    "    # Calculate and print the minimum distance between cluster means\n",
    "    min_distance = np.inf\n",
    "    for i in range(K):\n",
    "        for j in range(i+1, K):\n",
    "            distance = np.linalg.norm(posterior_mu[i] - posterior_mu[j])\n",
    "            min_distance = min(min_distance, distance)\n",
    "    print(f\"Minimum distance between cluster means: {min_distance:.3f}\")\n",
    "\n",
    "    _, probabilities_posterior = sample_z(data, posterior_pi, posterior_mu, posterior_Sigma, K)\n",
    "    rgb_colors_posterior = probabilities_to_rgb(probabilities_posterior, fixed_palette)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=rgb_colors_posterior, marker='o')\n",
    "\n",
    "    # Add cutoff lines\n",
    "    add_cutoff_lines(plt.gca(), sbp_cutoffs, bmi_cutoffs)\n",
    "\n",
    "    for k in range(K):\n",
    "        plt.scatter(posterior_mu[k, 0], posterior_mu[k, 1], c='black', marker='x', s=250, linewidths=4.5)\n",
    "        plt.scatter(posterior_mu[k, 0], posterior_mu[k, 1], c=[fixed_palette[k]], marker='x', s=120, linewidths=2.0)\n",
    "        plot_ellipse(plt.gca(), posterior_mu[k], posterior_Sigma[k], color=fixed_palette[k])\n",
    "\n",
    "    for k in range(K):\n",
    "        plt.scatter([], [], c=[fixed_palette[k]], marker='x', s=120, label=f'Cluster {k+1} Mean')\n",
    "\n",
    "    plt.title(\n",
    "    f'Posterior Means of Cluster Centers with constraint\\n Incorporating Epidemiological Knowledge {title} \\n'\n",
    "    f'($\\\\nu_0$={nu0}, $S_0$=[[{int(S0[0,0])}, {int(S0[0,1])}], '\n",
    "    f'[{int(S0[1,0])}, {int(S0[1,1])}]])', fontsize=18)\n",
    "\n",
    "    plt.xlabel(x_axis, fontsize=13)\n",
    "    plt.ylabel(y_axis, fontsize=13)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mu_metropolis_h2(data, z, K, V0, m0, Sigma, mu_current, sbp_ranges, bmi_ranges, sigma_proposal, h):\n",
    "    N, D = data.shape\n",
    "    mu_proposed = np.zeros((K, D))\n",
    "    \n",
    "    # Proposal distribution (multivariate normal distribution)\n",
    "    for k in range(K):\n",
    "        mu_proposed[k] = multivariate_normal.rvs(mean=mu_current[k], cov=sigma_proposal * np.eye(D))\n",
    "    \n",
    "    # Compute the log-posterior for the current and proposed means\n",
    "    log_posterior_current = 0\n",
    "    log_posterior_proposed = 0\n",
    "    \n",
    "    for k in range(K):\n",
    "        # Log-prior\n",
    "        log_posterior_current += multivariate_normal.logpdf(mu_current[k], mean=m0, cov=V0)\n",
    "        log_posterior_proposed += multivariate_normal.logpdf(mu_proposed[k], mean=m0, cov=V0)\n",
    "        \n",
    "        # Log-likelihood\n",
    "        for i in range(N):\n",
    "            if z[i] == k:\n",
    "                log_posterior_current += multivariate_normal.logpdf(data[i], mean=mu_current[k], cov=Sigma[k])\n",
    "                log_posterior_proposed += multivariate_normal.logpdf(data[i], mean=mu_proposed[k], cov=Sigma[k])\n",
    "    \n",
    "    # Add separation constraint term\n",
    "    h_current = h(mu_current, sbp_ranges, bmi_ranges)\n",
    "    h_proposed = h(mu_proposed, sbp_ranges, bmi_ranges)\n",
    "\n",
    "    if h_current == 0:\n",
    "        log_posterior_current = -np.inf\n",
    "    else:\n",
    "        log_posterior_current += np.log(h_current)\n",
    "    \n",
    "    if h_proposed == 0:\n",
    "        log_posterior_proposed = -np.inf\n",
    "    else:\n",
    "        log_posterior_proposed += np.log(h_proposed)\n",
    "\n",
    "    # Compute the acceptance ratio\n",
    "    acceptance_ratio = log_posterior_proposed - log_posterior_current\n",
    "    \n",
    "    if np.isnan(acceptance_ratio):\n",
    "        print(f\"NaN encountered in acceptance_ratio calculation: log_posterior_proposed={log_posterior_proposed}, log_posterior_current={log_posterior_current}\")\n",
    "\n",
    "\n",
    "    # Accept or reject the proposal\n",
    "    if np.log(np.random.uniform(0, 1)) <= acceptance_ratio:\n",
    "        mu = mu_proposed\n",
    "    else:\n",
    "        mu = mu_current\n",
    "    \n",
    "    return mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampling_gmm_h2(data, K, alpha, m0, V0, S0, nu0, initialization_function, sbp_ranges, bmi_ranges, sigma_proposal, num_iterations, h, s=None):\n",
    "    N, D = data.shape\n",
    "    \n",
    "    # Initialize variables\n",
    "\n",
    "    z, pi, mu, Sigma = initialization_function(data, K, sbp_ranges, bmi_ranges, s)\n",
    "    \n",
    "    samples = {'z': [], 'probabilities': [], 'pi': [], 'mu': [], 'Sigma': [], 'log_likelihood': []}\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Sample z\n",
    "        z, probabilities = sample_z(data, pi, mu, Sigma, K)\n",
    "        \n",
    "        # Sample pi\n",
    "        pi = sample_pi(z, K, alpha)\n",
    "        \n",
    "        # Sample mu using Metropolis within Gibbs\n",
    "        mu = sample_mu_metropolis_h2(data, z, K, V0, m0, Sigma, mu, sbp_ranges, bmi_ranges, sigma_proposal, h)\n",
    "        \n",
    "        # Sample Sigma\n",
    "        Sigma = sample_Sigma(data, z, K, mu, S0, nu0)\n",
    "        \n",
    "        # Store log-likelihood\n",
    "        log_likelihood = compute_log_likelihood(data, pi, mu, Sigma, K)\n",
    "        samples['log_likelihood'].append(log_likelihood)\n",
    "        \n",
    "        # Append current samples to the list\n",
    "        samples['z'].append(z.copy())\n",
    "        samples['probabilities'].append(probabilities.copy())\n",
    "        samples['pi'].append(pi.copy())\n",
    "        samples['mu'].append(mu.copy())\n",
    "        samples['Sigma'].append(Sigma.copy())\n",
    "    \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sbp = mean_US[1]\n",
    "std_sbp = std_US[1]\n",
    "mean_bmi = mean_US[0]\n",
    "std_bmi = std_US[0]\n",
    "\n",
    "# Function to normalize cutoffs\n",
    "def normalize_cutoffs(cutoffs, mean, std):\n",
    "    normalized_cutoffs = [( (low - mean) / std, (high - mean) / std if high != np.inf else np.inf) for (low, high) in cutoffs]\n",
    "    return normalized_cutoffs\n",
    "\n",
    "# Cutoff ranges for SBP and BMI\n",
    "sbp_cutoff = [(float('-inf'), 140), (140, 160), (160, float('inf'))]\n",
    "bmi_cutoff = [(float('-inf'),18.5),(18.5, 25), (25, 30), (30, float('inf'))]\n",
    "sbp_cutoffs = normalize_cutoffs(sbp_cutoff, mean_sbp, std_sbp)\n",
    "bmi_cutoffs = normalize_cutoffs(bmi_cutoff, mean_bmi, std_bmi)\n",
    "\n",
    "\n",
    "z, pi, mu, Sigma = initialize_parameters_random_subset_blocks(data_norm_US, K, sbp_cutoffs, bmi_cutoffs, s=452)\n",
    "\n",
    "\n",
    "# Plot the initial cluster means and cutoff lines\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(data_norm_US[:, 0], data_norm_US[:, 1], c='gray', alpha=0.5, label='Data points')\n",
    "plt.scatter(mu[:, 0], mu[:, 1], c='red', marker='x', s=100, label='Cluster means')\n",
    "\n",
    "# Plot cutoff lines for BMI\n",
    "for low, high in bmi_cutoffs:\n",
    "    if high != np.inf:\n",
    "        plt.axvline(x=low, linestyle='--', color='green')\n",
    "        plt.axvline(x=high, linestyle='--', color='green')\n",
    "    else:\n",
    "        plt.axvline(x=low, linestyle='--', color='green')\n",
    "\n",
    "# Plot cutoff lines for SBP\n",
    "for low, high in sbp_cutoffs:\n",
    "    if high != np.inf:\n",
    "        plt.axhline(y=low, linestyle='--', color='blue')\n",
    "        plt.axhline(y=high, linestyle='--', color='blue')\n",
    "    else:\n",
    "        plt.axhline(y=low, linestyle='--', color='blue')\n",
    "\n",
    "plt.xlabel('Normalized BMI')\n",
    "plt.ylabel('Normalized SBP')\n",
    "plt.legend()\n",
    "plt.title('Initial Cluster Means and Cutoff Ranges')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Gibbs sampling on US data with h2 \n",
    "S0 = np.eye(2)\n",
    "nu0 = 4\n",
    "sigma_proposal=0.001\n",
    "samples_blocks = gibbs_sampling_gmm_h2(data_norm_US, K, alpha, m0, V0, S0, nu0, initialize_parameters_random_subset_blocks, sbp_ranges=sbp_cutoffs, bmi_ranges=bmi_cutoffs, sigma_proposal=sigma_proposal, num_iterations=3000, h=h2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_iterations_lines(samples_blocks, [1300, 2000], data_norm_US, K, fixed_palette, S0, nu0, 'Incorporating Constraint with Epidemiological Knowledge (NHANES Dataset)\\n'f'($\\\\nu_0$={nu0}, $S_0$=[[{int(S0[0,0])}, {int(S0[0,1])}], '\n",
    "    f'[{int(S0[1,0])}, {int(S0[1,1])}]])', 'BMI', 'SBP', sbp_cutoffs, bmi_cutoffs)\n",
    "plot_clusters_posterior_h_lines(data_norm_US, samples_blocks, K, fixed_palette, S0, nu0, '(NHANES Dataset)', 'BMI', 'SBP', sbp_cutoffs, bmi_cutoffs, burn_in=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S0 = 6* np.eye(2)\n",
    "nu0 = 50\n",
    "\n",
    "samples_blocks_2 = gibbs_sampling_gmm_h2(data_norm_US, K, alpha, m0, V0, S0 , nu0, initialize_parameters_random_subset_blocks, sbp_ranges=sbp_cutoffs, bmi_ranges=bmi_cutoffs, sigma_proposal=sigma_proposal, num_iterations=3000, h=h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_iterations_lines(samples_blocks_2, [700, 2950], data_norm_US, K, fixed_palette, S0, nu0, 'Incorporating Constraint with Epidemiological Knowledge (NHANES Dataset)\\n'f'($\\\\nu_0$={nu0}, $S_0$=[[{int(S0[0,0])}, {int(S0[0,1])}], '\n",
    "    f'[{int(S0[1,0])}, {int(S0[1,1])}]])', 'BMI', 'SBP', sbp_cutoffs, bmi_cutoffs)\n",
    "plot_clusters_posterior_h_lines(data_norm_US, samples_blocks_2, K, fixed_palette, S0, nu0, '(NHANES Dataset)', 'BMI', 'SBP', sbp_cutoffs, bmi_cutoffs, burn_in=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the data (adjusted to include more variables)\n",
    "df = pd.read_csv('NHANES_adults_data_preproc.csv')\n",
    "variables = ['bmi', 'sbp', 'non_hdl','hba1c']  \n",
    "df_variables = df[variables]\n",
    "\n",
    "# Check for missing values in the selected columns\n",
    "missing_values = df_variables.isnull().sum()\n",
    "print(\"Missing values before removal:\")\n",
    "print(missing_values)\n",
    "\n",
    "df_variables_clean = df_variables.dropna()\n",
    "\n",
    "# Normalize the entire dataset\n",
    "data_US = df_variables_clean.values\n",
    "data_norm_US_full, mean_US, std_US = normalize_data(data_US)\n",
    "\n",
    "data_norm_US_full_df = pd.DataFrame(data_norm_US_full, columns=variables)\n",
    "\n",
    "# Take a random subset of the normalized data\n",
    "np.random.seed(5)\n",
    "data_norm_US = data_norm_US_full_df.sample(n=300, replace=False, random_state=5)\n",
    "data_norm_US\n",
    "\n",
    "\n",
    "data_norm_US = data_norm_US.values\n",
    "data_norm_US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plots\n",
    "def plot_trace_plots(samples, num_iterations, K, S0, nu0):\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(8, 14))\n",
    "\n",
    "    # Plot trace of pi\n",
    "    for k in range(K):\n",
    "        axes[0].plot([samples['pi'][i][k] for i in range(num_iterations)], label=f'pi_{k}')\n",
    "    axes[0].set_title(f'Trace plot of pi with S0={S0}, nu0={nu0}')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot trace of mu\n",
    "    for k in range(K):\n",
    "        for d in range(samples['mu'][0].shape[1]):\n",
    "            axes[1].plot([samples['mu'][i][k, d] for i in range(num_iterations)], label=f'mu_{k}_{d}')\n",
    "    axes[1].set_title(f'Trace plot of mu with S0={S0}, nu0={nu0}')\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Plot trace of Sigma (diagonal elements only)\n",
    "    for k in range(K):\n",
    "        for d in range(samples['Sigma'][0].shape[1]):\n",
    "            axes[2].plot([samples['Sigma'][i][k, d, d] for i in range(num_iterations)], label=f'Sigma_{k}_{d}_{d}')\n",
    "    axes[2].set_title(f'Trace plot of Sigma (diagonal elements) with S0={S0}, nu0={nu0}')\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update to handle the constraints for multiple variables\n",
    "def initialize_parameters_random_subset_blocks_multid(data, K, variable_ranges, s=None):\n",
    "    if s is not None:\n",
    "        np.random.seed(s)\n",
    "    N, D = data.shape\n",
    "    mu = np.zeros((K, D))\n",
    "    valid = False\n",
    "    \n",
    "    while not valid:\n",
    "        random_indices = np.random.choice(N, K, replace=False)\n",
    "        mu = data[random_indices]\n",
    "        \n",
    "        if h2_multid(mu, variable_ranges) == 1:\n",
    "            valid = True\n",
    "    \n",
    "    z = np.zeros(N, dtype=int)\n",
    "    for i in range(N):\n",
    "        distances = np.linalg.norm(data[i] - mu, axis=1)\n",
    "        z[i] = np.argmin(distances)\n",
    "    \n",
    "    Sigma = np.zeros((K, D, D))\n",
    "    for k in range(K):\n",
    "        cluster_data = data[z == k]\n",
    "        if len(cluster_data) > 1:\n",
    "            Sigma[k] = np.cov(cluster_data, rowvar=False)\n",
    "        else:\n",
    "            Sigma[k] = np.eye(D)\n",
    "    \n",
    "    pi = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        pi[k] = np.sum(z == k) / N\n",
    "    \n",
    "    return z, pi, mu, Sigma\n",
    "\n",
    "\n",
    "def h2_multid(mu, ranges):\n",
    "    K = len(mu)  # Number of clusters\n",
    "    num_vars = mu.shape[1]  # Number of variables (dimensions)\n",
    "    \n",
    "\n",
    "    unique_combinations = set()\n",
    "    \n",
    "    for k in range(K):\n",
    "        combination = []\n",
    "        \n",
    "        for dim in range(num_vars):\n",
    "            value = mu[k, dim]\n",
    "            dimension_ranges = ranges[dim]\n",
    "            range_index = None\n",
    "            \n",
    "    \n",
    "            for idx, (low, high) in enumerate(dimension_ranges):\n",
    "                if low <= value < high:\n",
    "                    range_index = idx\n",
    "                    break\n",
    "            \n",
    "            if range_index is None:\n",
    "                print(f\"Value {value} for dimension {dim} does not fall into any range.\")\n",
    "                return 0  # Return 0 if the value doesn't fall into any range\n",
    "            \n",
    "            combination.append(range_index)\n",
    "        \n",
    "        combination_tuple = tuple(combination)\n",
    "        \n",
    "  \n",
    "        \n",
    "        unique_combinations.add(combination_tuple)\n",
    "    \n",
    "    return 1 \n",
    "\n",
    "# Update to handle multiple variables\n",
    "def sample_mu_metropolis_h2_multid(data, z, K, V0, m0, Sigma, mu_current, variable_ranges, sigma_proposal, h):\n",
    "    N, D = data.shape\n",
    "    mu_proposed = np.zeros((K, D))\n",
    "    \n",
    "    for k in range(K):\n",
    "        mu_proposed[k] = multivariate_normal.rvs(mean=mu_current[k], cov=sigma_proposal * np.eye(D))\n",
    "    \n",
    "    log_posterior_current = 0\n",
    "    log_posterior_proposed = 0\n",
    "    \n",
    "    for k in range(K):\n",
    "        log_posterior_current += multivariate_normal.logpdf(mu_current[k], mean=m0, cov=V0)\n",
    "        log_posterior_proposed += multivariate_normal.logpdf(mu_proposed[k], mean=m0, cov=V0)\n",
    "        \n",
    "        for i in range(N):\n",
    "            if z[i] == k:\n",
    "                log_posterior_current += multivariate_normal.logpdf(data[i], mean=mu_current[k], cov=Sigma[k])\n",
    "                log_posterior_proposed += multivariate_normal.logpdf(data[i], mean=mu_proposed[k], cov=Sigma[k])\n",
    "    \n",
    "    h_current = h(mu_current, variable_ranges)\n",
    "    h_proposed = h(mu_proposed, variable_ranges)\n",
    "\n",
    "    if h_current == 0:\n",
    "        log_posterior_current = -np.inf\n",
    "    else:\n",
    "        log_posterior_current += np.log(h_current)\n",
    "    \n",
    "    if h_proposed == 0:\n",
    "        log_posterior_proposed = -np.inf\n",
    "    else:\n",
    "        log_posterior_proposed += np.log(h_proposed)\n",
    "\n",
    "    acceptance_ratio = log_posterior_proposed - log_posterior_current\n",
    "    \n",
    "    if np.isnan(acceptance_ratio):\n",
    "        print(f\"NaN encountered in acceptance_ratio calculation: log_posterior_proposed={log_posterior_proposed}, log_posterior_current={log_posterior_current}\")\n",
    "\n",
    "    if np.log(np.random.uniform(0, 1)) <= acceptance_ratio:\n",
    "        mu = mu_proposed\n",
    "    else:\n",
    "        mu = mu_current\n",
    "    \n",
    "    return mu\n",
    "\n",
    "\n",
    "\n",
    "def gibbs_sampling_gmm_h2_multid(data, K, alpha, m0, V0, S0, nu0, initialization_function, variable_ranges, sigma_proposal, num_iterations, h, s=None):\n",
    "    N, D = data.shape\n",
    "    \n",
    "    # Initialize variables\n",
    "    z, pi, mu, Sigma = initialization_function(data, K, variable_ranges, s)\n",
    "    \n",
    "    samples = {'z': [], 'probabilities': [], 'pi': [], 'mu': [], 'Sigma': [], 'log_likelihood': []}\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Sample z\n",
    "        z, probabilities = sample_z(data, pi, mu, Sigma, K)\n",
    "        \n",
    "        # Sample pi\n",
    "        pi = sample_pi(z, K, alpha)\n",
    "        \n",
    "        # Sample mu using Metropolis within Gibbs\n",
    "        mu = sample_mu_metropolis_h2_multid(data, z, K, V0, m0, Sigma, mu, variable_ranges, sigma_proposal, h)\n",
    "        \n",
    "        # Sample Sigma\n",
    "        Sigma = sample_Sigma(data, z, K, mu, S0, nu0)\n",
    "        \n",
    "        # Store log-likelihood\n",
    "        log_likelihood = compute_log_likelihood(data, pi, mu, Sigma, K)\n",
    "        samples['log_likelihood'].append(log_likelihood)\n",
    "        \n",
    "        # Append current samples to the list\n",
    "        samples['z'].append(z.copy())\n",
    "        samples['probabilities'].append(probabilities.copy())\n",
    "        samples['pi'].append(pi.copy())\n",
    "        samples['mu'].append(mu.copy())\n",
    "        samples['Sigma'].append(Sigma.copy())\n",
    "    \n",
    "    return samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cutoff ranges for the selected variables\n",
    "cutoff_ranges = {\n",
    "    'bmi': [(float('-inf'),18.5),(18.5, 25), (25, 30), (30, float('inf'))],\n",
    "    'sbp': [(float('-inf'), 140), (140, 160), (160, float('inf'))],  \n",
    "    'non_hdl': [(float('-inf'), 3.36), (3.36, 5.69), (5.69, float('inf'))], \n",
    "    'hba1c': [(float('-inf'), 6.5), (6.5, 10.0), (10.0, float('inf'))],\n",
    "}\n",
    "\n",
    "\n",
    "mean_std_values = {\n",
    "    'bmi': (mean_US[0], std_US[0]),\n",
    "    'sbp': (mean_US[1], std_US[1]),  \n",
    "    'non_hdl': (mean_US[2], std_US[2]),  \n",
    "    'hba1c': (mean_US[3], std_US[3]), \n",
    "}\n",
    "\n",
    "# Apply normalization to the cutoff ranges\n",
    "normalized_cutoff_ranges = {var: normalize_cutoffs(cutoff, mean_std_values[var][0], mean_std_values[var][1]) \n",
    "                            for var, cutoff in cutoff_ranges.items()}\n",
    "\n",
    "# Convert to list of ranges for the Gibbs sampling function\n",
    "variable_ranges = list(normalized_cutoff_ranges.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "def plot_posterior_distributions(samples, variables, K, population_distributions, title, burn_in=0, mean_std_values=None, cutoff_ranges=None):\n",
    "\n",
    "    mu_samples = samples['mu'][burn_in:]\n",
    "\n",
    "    num_iterations = len(mu_samples)\n",
    "    num_variables = len(variables)\n",
    "\n",
    "    # Define color palette for clusters\n",
    "    colors = sns.color_palette(\"husl\", K)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))  \n",
    "    fig.suptitle(title, fontsize=20)\n",
    "\n",
    "\n",
    "    x_labels = {\n",
    "        'bmi': 'BMI (Body Mass Index)',\n",
    "        'sbp': 'SBP (Blood Systolic Pressure)',\n",
    "        'non_hdl': 'Non-HDL (Non-high-density lipoprotein cholesterol)',\n",
    "        'hba1c': 'HbA1c (Glycated Hemoglobin)'\n",
    "    }\n",
    "\n",
    "    labels = {\n",
    "        'bmi': 'BMI',\n",
    "        'sbp': 'SBP',\n",
    "        'non_hdl': 'Non-HDL',\n",
    "        'hba1c': 'HbA1c'\n",
    "    }\n",
    "\n",
    "    for var_idx, var_name in enumerate(variables):\n",
    " \n",
    "        row, col = divmod(var_idx, 2)\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        for k in range(K):\n",
    "            means = [mu_samples[i][k, var_idx] for i in range(num_iterations)]\n",
    "            \n",
    "            # Transform the means back to original scale\n",
    "            original_means = [(mean_std_values[var_name][0] + m * mean_std_values[var_name][1]) for m in means]\n",
    "            \n",
    "            sns.kdeplot(original_means, ax=ax, color=colors[k], lw=2, label=f'Cluster {k+1}')\n",
    "\n",
    "        x = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 100)\n",
    "        population_density = population_distributions[var_idx](x)\n",
    "        ax.plot(x, population_density, color='red', linestyle='--', alpha=0.5, \n",
    "                label=f'Population Distribution for {labels[var_name]}')\n",
    "\n",
    "        ax.set_xlabel(x_labels.get(var_name, var_name), fontsize=12)  \n",
    "        ax.set_ylabel('Density',fontsize=12)\n",
    "        ax.legend()\n",
    " \n",
    "        if cutoff_ranges and var_name in cutoff_ranges:\n",
    "            for (low, high) in cutoff_ranges[var_name]:\n",
    "                if low != -np.inf:\n",
    "                    original_low = mean_std_values[var_name][0] + low * mean_std_values[var_name][1]\n",
    "                    ax.axvline(original_low, color='black', linestyle='--', alpha=0.7)\n",
    "                if high != np.inf:\n",
    "                    original_high = mean_std_values[var_name][0] + high * mean_std_values[var_name][1]\n",
    "                    ax.axvline(original_high, color='black', linestyle='--', alpha=0.7)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "population_distributions = []\n",
    "for var_idx in range(data_US.shape[1]):\n",
    "    kde = gaussian_kde(data_US[:, var_idx])\n",
    "    population_distributions.append(kde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_posterior_means(samples_h2, burn_in):\n",
    "    num_iterations_after_burnin = len(samples_h2['mu']) - burn_in\n",
    "    mu_samples = np.array(samples_h2['mu'][burn_in:])\n",
    "    pi_samples = np.array(samples_h2['pi'][burn_in:])\n",
    "    Sigma_samples = np.array(samples_h2['Sigma'][burn_in:])\n",
    "    # Compute posterior mean for mu\n",
    "    posterior_mean_mu = np.mean(mu_samples, axis=0)\n",
    "    # Compute posterior mean for pi\n",
    "    posterior_mean_pi = np.mean(pi_samples, axis=0)\n",
    "    # Compute posterior mean for Sigma\n",
    "    posterior_mean_Sigma = np.mean(Sigma_samples, axis=0)\n",
    "    return posterior_mean_mu, posterior_mean_pi, posterior_mean_Sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the Gibbs sampling\n",
    "K = 5\n",
    "alpha = np.ones(K)\n",
    "m0 = np.zeros(len(variables)) \n",
    "V0 = np.eye(len(variables))\n",
    "nu0 = len(variables) + 2\n",
    "sigma_proposal = 0.001\n",
    "S0 =  np.eye(len(variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_standard = gibbs_sampling_gmm(\n",
    "    data_norm_US, \n",
    "    K, \n",
    "    alpha, \n",
    "    m0, \n",
    "    V0, \n",
    "    S0, \n",
    "    nu0, \n",
    "    initialize_parameters_random_subset,\n",
    "    num_iterations=3500\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posterior_mean_mu_standard, posterior_mean_pi_standard, posterior_mean_Sigma_standard = compute_posterior_means(samples_standard,200)\n",
    "print(\"\\nPosterior Mean of pi:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior_distributions(\n",
    "    samples=samples_standard,  \n",
    "    variables=variables,\n",
    "    K=K,\n",
    "    population_distributions=population_distributions,\n",
    "    title='Posterior Distributions of Cluster Centers Across Variables',\n",
    "    burn_in=200,\n",
    "    mean_std_values=mean_std_values,\n",
    "    cutoff_ranges=normalized_cutoff_ranges,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_h1 = gibbs_sampling_gmm_h1(\n",
    "    data_norm_US, \n",
    "    K, \n",
    "    alpha, \n",
    "    m0, \n",
    "    V0, \n",
    "    S0, \n",
    "    nu0, \n",
    "    initialize_parameters_random_subset,  # Initialization without constraints\n",
    "    sigma_proposal, \n",
    "    num_iterations=3500, \n",
    "    h=h1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posterior_mean_mu_h1, posterior_mean_pi_h1, posterior_mean_Sigma_h1 = compute_posterior_means(samples_h1,200)\n",
    "print(\"\\nPosterior Mean of pi:\")\n",
    "print(posterior_mean_pi_h1)\n",
    "\n",
    "plot_posterior_distributions(\n",
    "    samples=samples_h1,  \n",
    "    variables=variables,\n",
    "    K=K,\n",
    "    population_distributions=population_distributions,\n",
    "    title='Posterior Distributions of Cluster Centers Across Variables \\n(GMM with Minimum Distance Maximization Constraint)',\n",
    "    burn_in=200,\n",
    "    mean_std_values=mean_std_values,\n",
    "    cutoff_ranges=normalized_cutoff_ranges\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples_h2 = gibbs_sampling_gmm_h2_multid(\n",
    "    data_norm_US, \n",
    "    K, \n",
    "    alpha, \n",
    "    m0, \n",
    "    V0, \n",
    "    S0, \n",
    "    nu0, \n",
    "    initialize_parameters_random_subset_blocks_multid, \n",
    "    variable_ranges, \n",
    "    sigma_proposal, \n",
    "    num_iterations=3500, \n",
    "    h=h2_multid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "posterior_mean_mu_h2, posterior_mean_pi_h2, posterior_mean_Sigma_h2 = compute_posterior_means(samples_h2,200)\n",
    "print(\"\\nPosterior Mean of pi:\")\n",
    "print(posterior_mean_pi_h2)\n",
    "\n",
    "plot_posterior_distributions(\n",
    "    samples=samples_h2, \n",
    "    variables=variables,\n",
    "    K=K,\n",
    "    population_distributions=population_distributions,\n",
    "    title='Posterior Distributions of Cluster Centers Across Variables \\n(GMM Incorporating Epidemiological Knowledge)',\n",
    "    burn_in=200,\n",
    "    mean_std_values=mean_std_values,\n",
    "    cutoff_ranges=normalized_cutoff_ranges,\n",
    "\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
